!pip -q install requests beautifulsoup4 pandas lxmlimport re, timeimport requestsimport pandas as pdfrom bs4 import BeautifulSoupBASE_URL = "https://webtai.bipm.org/ftp/pub/tai/Circular-T/cirthtm/"def _clean_cell(s: str) -> str:    if s is None:        return ""    s = s.strip().replace("−","-").replace("–","-").replace("—","-").replace("\xa0"," ")    return s.strip()_num_re = re.compile(r"^[+-]?\d+(?:\.\d+)?$")def _to_float_or_none(s: str):    s = _clean_cell(s)    if not s:        return None    s = s.replace(",", "")    if s in {"-", "--"}:        return None    if _num_re.match(s):        try:            return float(s)        except:            return None    return Nonesess = requests.Session()sess.headers.update({"User-Agent": "clock-coherence-forecast-scraper/1.0"})def fetch_html(cirt_num: int) -> str | None:    url = f"{BASE_URL}cirt.{cirt_num}.html"    r = sess.get(url, timeout=30)    if r.status_code == 404:        return None    r.raise_for_status()    return r.textdef parse_utc_utck_ns_table(html: str):    soup = BeautifulSoup(html, "lxml")    target = None    for table in soup.find_all("table"):        txt = _clean_cell(table.get_text(" ")).upper().replace(" ", "")        if "UTC-UTC" in txt and "NS" in txt:            target = table            break    if target is None:        return [], {}    grid = []    for tr in target.find_all("tr"):        cells = tr.find_all(["th","td"])        row = [_clean_cell(c.get_text(" ", strip=True)) for c in cells]        if any(x for x in row):            grid.append(row)    if not grid:        return [], {}    header_idx = None    header_mjds = []    for i, row in enumerate(grid):        mjds = []        for cell in row:            cell2 = cell.replace(" ", "")            if re.fullmatch(r"\d{5}", cell2):                mjds.append(float(cell2))        if len(mjds) >= 3:            header_idx = i            header_mjds = mjds            break    if header_idx is None or not header_mjds:        return [], {}    mjds = header_mjds    n = len(mjds)    rows = {}    for row in grid[header_idx+1:]:        if len(row) < 2:            continue        lab = row[0].strip().upper()        if not lab or not re.fullmatch(r"[A-Z0-9]{2,6}", lab):            continue        vals = []        for c in row[1:1+n]:            vals.append(_to_float_or_none(c))        if len(vals) < n:            vals += [None]*(n-len(vals))        if any(v is not None for v in vals):            rows[lab] = vals    return mjds, rowsdef build_csv(start: int, end: int, labs=("USNO","PTB","NIST"), out_prefix="circularT", sleep_s=0.25):    labs = tuple(l.upper() for l in labs)    out = {lab: [] for lab in labs}    for cirt in range(start, end+1):        html = fetch_html(cirt)        if html is None:            print(f"[SKIP] {cirt}: 404")            continue        mjds, rows = parse_utc_utck_ns_table(html)        if not mjds or not rows:            print(f"[WARN] {cirt}: couldn't parse main table (mjds={len(mjds)}, rows={len(rows)})")            time.sleep(sleep_s)            continue        for lab in labs:            if lab not in rows:                continue            for mjd, val in zip(mjds, rows[lab]):                if val is None:                    continue                out[lab].append({"mjd": float(mjd), "value_ns": float(val), "cirt": int(cirt)})        print(f"[OK] {cirt}: parsed (mjds={len(mjds)}, labs_in_table={len(rows)})")        time.sleep(sleep_s)    for lab in labs:        df = pd.DataFrame(out[lab]).drop_duplicates().sort_values(["mjd","cirt"])        if df.empty:            print(f"[WARN] No rows collected for {lab}.")            continue        fname = f"{out_prefix}_{lab}_extracted.csv"        df.to_csv(fname, index=False)        print(f"[DONE] wrote {fname} rows={len(df)}")build_csv(337, 456, labs=("USNO","PTB","NIST","NPL","NICT","INRIM","VSL"))